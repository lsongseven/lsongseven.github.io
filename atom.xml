<?xml version="1.0" encoding="utf-8"?>


<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title type="text">Blog of Tinys</title>
    <subtitle type="html">MemE is a powerful and highly customizable GoHugo theme for personal blogs.</subtitle>
    <updated>2020-08-29T13:45:21&#43;08:00</updated>
    <id>https://lsongseven.github.io/</id>
    <link rel="alternate" type="text/html" href="https://lsongseven.github.io/" />
    <link rel="self" type="application/atom&#43;xml" href="https://lsongseven.github.io/atom.xml" />
    <author>
            <name>lsongseven</name>
            <uri>https://lsongseven.github.io/</uri>
            
                <email>lsongseven@gmail.com</email>
            </author>
    <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights>
    <generator uri="https://gohugo.io/" version="0.74.3">Hugo</generator>
        <entry>
            <title type="text">Java进程容器内存监控指标</title>
            <link rel="alternate" type="text/html" href="https://lsongseven.github.io/posts/java-container-metrics/" />
            <id>https://lsongseven.github.io/posts/java-container-metrics/</id>
            <updated>2020-08-29T13:34:53&#43;08:00</updated>
            <published>2020-08-28T22:58:25&#43;08:00</published>
            <author>
                    <name>lsongseven</name>
                    <uri>https://github.com/lsongseven</uri>
                    <email>lsongseven@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">以container_memory_usage_bytes作为memory指标 过去的一段时间内，xxx服务在prometheus上面用来监控内存的指标是以容器内存使用量（率）来衡量的，如下：
container_memory_usage_bytes{pod=~&amp;quot;xxx.*-[^-]*-[^-]*$&amp;quot;, image!=&amp;quot;&amp;quot;, container!=&amp;quot;POD&amp;quot;}*100/container_spec_memory_limit_bytes{pod=~&amp;quot;xxx.*-[^-]*-[^-]*$&amp;quot;, image!=&amp;quot;&amp;quot;, container!=&amp;quot;POD&amp;quot;} &amp;gt; 80.0 xxx服务在kubernetes中的资源配置情况如下，
resources: limits: cpu: &amp;quot;2&amp;quot; memory: &amp;quot;4Gi&amp;quot; requests: cpu: &amp;quot;100m&amp;quot; memory: &amp;quot;512Mi&amp;quot; 在这样的配置下，上面监控rule的语义就是，当容器内使用内存量达到容器最大内存（4G）的80%时发出告警。
再来看下container_memory_usage_bytes如何定义，在cadvisor上面一个issue中提到：
container_memory_usage_bytes == container_memory_rss + container_memory_cache + container_memory_swap + kernel memory(issue中提到这里的kernel memory还未被暴露为metrics).
看到rss, swap, cache这些指标很自然的就想到top，在top中也有一个指标叫做used，其定义为 ：“USED - simply the sum of RES and SWAP”, https://man7.</summary>
            
                <content type="html">&lt;h1 id=&#34;以container_memory_usage_bytes作为memory指标&#34;&gt;以container_memory_usage_bytes作为memory指标&lt;/h1&gt;
&lt;p&gt;过去的一段时间内，xxx服务在prometheus上面用来监控内存的指标是以容器内存使用量（率）来衡量的，如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;container_memory_usage_bytes{pod=~&amp;quot;xxx.*-[^-]*-[^-]*$&amp;quot;, image!=&amp;quot;&amp;quot;, container!=&amp;quot;POD&amp;quot;}*100/container_spec_memory_limit_bytes{pod=~&amp;quot;xxx.*-[^-]*-[^-]*$&amp;quot;, image!=&amp;quot;&amp;quot;, container!=&amp;quot;POD&amp;quot;} &amp;gt; 80.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;xxx服务在kubernetes中的资源配置情况如下，&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;resources:
  limits:
    cpu: &amp;quot;2&amp;quot;
    memory: &amp;quot;4Gi&amp;quot;
  requests:
    cpu: &amp;quot;100m&amp;quot;
    memory: &amp;quot;512Mi&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在这样的配置下，上面监控rule的语义就是，当容器内使用内存量达到容器最大内存（4G）的80%时发出告警。&lt;/p&gt;
&lt;p&gt;再来看下container_memory_usage_bytes如何定义，在cadvisor上面一个issue中提到：&lt;/p&gt;
&lt;p&gt;container_memory_usage_bytes == container_memory_rss + container_memory_cache + container_memory_swap + kernel memory(issue中提到这里的kernel memory还未被暴露为metrics).&lt;/p&gt;
&lt;p&gt;看到rss, swap, cache这些指标很自然的就想到top，在top中也有一个指标叫做used，其定义为 ：“USED - simply the sum of RES and SWAP”, &lt;a href=&#34;https://man7.org/linux/man-pages/man1/top.1.html&#34;&gt;https://man7.org/linux/man-pages/man1/top.1.html&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;以container_memory_usage_bytes作为java容器内存指标的问题&#34;&gt;以container_memory_usage_bytes作为java容器内存指标的问题&lt;/h1&gt;
&lt;h2 id=&#34;频繁告警&#34;&gt;频繁告警&lt;/h2&gt;
&lt;p&gt;某刻xxx服务中某个pod中top指标如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;top - 05:35:43 up 7 days, 21:32,  0 users,  load average: 11.99, 11.35, 8.50
Tasks:  10 total,   1 running,   9 sleeping,   0 stopped,   0 zombie
%Cpu(s):  6.7 us,  6.5 sy,  0.0 ni, 85.9 id,  0.3 wa,  0.0 hi,  0.7 si,  0.0 st
KiB Mem : 65960736 total,  7194880 free, 42443524 used, 16322332 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 19621872 avail Mem 

   PID %MEM    RES    VIRT    SHR   SWAP   CODE COMMAND                                               
     9  6.0   3.8g 9833416  16400      0      4 java                                                  
107812  0.0   2140   59728   1504      0     96 top                                                   
 69248  0.0   2152   59716   1504      0     96 top                                                   
 57442  0.0   2152   59712   1504      0     96 top                                                   
 57423  0.0   2992   16160   1624      0    888 bash                                                  
 68879  0.0   2928   16160   1564      0    888 bash                                                  
107795  0.0   2936   16160   1568      0    888 bash                                                  
 68899  0.0   3000   16156   1628      0    888 bash                                                  
     1  0.0   2548   16020   1328      0    888 bash                                                  
107946  0.0    476    7876    280      0     24 sleep
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到我们的9号java进程，RES占用有3.8GB，而前面说过，USED=RES+SWAP，这里还没有发生SWAP，所以USED等于RES占用了3.8GB。在我们的告警规则中，配置了当容器内存达到最大值4GB的80%时发生告警，即容器内USED &amp;gt; 3.2GB时会告警，容器内最重要java进程自己的USED就远大于这个阈值，所以会频繁告警。&lt;/p&gt;
&lt;p&gt;那么，RES为3.8GB有什么问题呢？&lt;/p&gt;
&lt;h2 id=&#34;jvm指标&#34;&gt;JVM指标&lt;/h2&gt;
&lt;p&gt;既然是java进程常驻内存很高，那么首先怀疑的就是jvm内存占用问题。&lt;/p&gt;
&lt;p&gt;看一下堆内存，jmap -heap 9，发现很健康&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Heap Configuration:
   MinHeapFreeRatio         = 40
   MaxHeapFreeRatio         = 70
   MaxHeapSize              = 3221225472 (3072.0MB)
   NewSize                  = 1073741824 (1024.0MB)
   MaxNewSize               = 1073741824 (1024.0MB)
   OldSize                  = 2147483648 (2048.0MB)
   NewRatio                 = 2
   SurvivorRatio            = 8
   MetaspaceSize            = 21807104 (20.796875MB)
   CompressedClassSpaceSize = 1073741824 (1024.0MB)
   MaxMetaspaceSize         = 17592186044415 MB
   G1HeapRegionSize         = 0 (0.0MB)


Heap Usage:
PS Young Generation
Eden Space:
   capacity = 859832320 (820.0MB)
   used     = 483407288 (461.0131149291992MB)
   free     = 376425032 (358.9868850708008MB)
   56.22111157673161% used
From Space:
   capacity = 106954752 (102.0MB)
   used     = 50262824 (47.934364318847656MB)
   free     = 56691928 (54.065635681152344MB)
   46.99447482239966% used
To Space:
   capacity = 106954752 (102.0MB)
   used     = 0 (0.0MB)
   free     = 106954752 (102.0MB)
   0.0% used
PS Old Generation
   capacity = 2147483648 (2048.0MB)
   used     = 143595616 (136.94345092773438MB)
   free     = 2003888032 (1911.0565490722656MB)
   6.68669193983078% used
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;不信邪手动gc一下，发现RES并没有什么变化。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jcmd 9 GC.run
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;到这里可以明白堆内内存没有什么问题，下一个怀疑对象是堆外内存。启动参数上加上-XX:NativeMemoryTracking=summary，然后jcmd 9 VM.native_memory summary scale=MB看一下发现依然很健康（虽然加上参数容器会重启，但下面数据也是在频繁告警的情况下获得的）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Native Memory Tracking:

Total: reserved=5261MB, committed=4124MB
-                 Java Heap (reserved=3072MB, committed=3072MB)
                            (mmap: reserved=3072MB, committed=3072MB) 
 
-                     Class (reserved=1180MB, committed=174MB)
                            (classes #25539)
                            (malloc=12MB #47472) 
                            (mmap: reserved=1168MB, committed=162MB) 
 
-                    Thread (reserved=535MB, committed=535MB)
                            (thread #531)
                            (stack: reserved=532MB, committed=532MB)
                            (malloc=2MB #2660) 
                            (arena=1MB #1057)
 
-                      Code (reserved=274MB, committed=144MB)
                            (malloc=30MB #29634) 
                            (mmap: reserved=244MB, committed=113MB) 
 
-                        GC (reserved=122MB, committed=122MB)
                            (malloc=10MB #615) 
                            (mmap: reserved=112MB, committed=112MB) 
 
-                  Compiler (reserved=1MB, committed=1MB)
                            (malloc=1MB #3104) 
 
-                  Internal (reserved=40MB, committed=40MB)
                            (malloc=40MB #38278) 
 
-                    Symbol (reserved=30MB, committed=30MB)
                            (malloc=26MB #267011) 
                            (arena=4MB #1)
 
-    Native Memory Tracking (reserved=6MB, committed=6MB)
                            (tracking overhead=6MB)

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;与前文的堆内内存相比较，容易发现这里所指的committed即capacity，那么offsetHeap的capacity=174+535+144+122+1+40+30+6 MB = 1052 MB，注意这里仅为committed(或者可以理解成capacity)，实际使用肯定是要小于这个值的，那么再考虑到前面heap的实际使用情况461+47+136 MB = 644 MB， 以上两者(heap+offsetHeap)之和肯定是要远远小于3.8g的。&lt;/p&gt;
&lt;p&gt;这里可以根据到这里可以发现jvm很健康，内存占用是比较低的。那么，top命令中的RES究竟是什么呢？&lt;/p&gt;
&lt;h2 id=&#34;重新考量top&#34;&gt;重新考量top&lt;/h2&gt;
&lt;p&gt;写到这里啰嗦了一大堆，前面的逻辑简单概括一下：&lt;/p&gt;
&lt;p&gt;使用了cadvisor里面的container_memory_usage_bytes作为指标参数，而这个指标和top强相关，都是RES+SWAP+一些其他东西（占用很小）；
在我们应用中没有发生swap，所以造成告警的原因主要是java进程的RES很大；
java进程内部堆内内存和堆外内存很健康，没有看到大量占用内存的迹象；
不得不说top是一个很强大的工具，但是他也有很多的诟病，尤其是对于java进程来说，下面摘录一些国外开发者的抱怨：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;quot;It really annoys me that there&amp;rsquo;s no good tool for measuring memory usage on Linux. There are tools, like &amp;lsquo;top&amp;rsquo;, but they often cause more harm than good - most people don&amp;rsquo;t even know what the fields really mean and only few people can interpret them correctly. Mind you, even I&amp;rsquo;m not sure I can, and in fact I sometimes doubt such person even exists. The problem is, even intepreting the numbers may not give the answer. Measuring memory usage on Linux is voodoo magic.&amp;ldquo;&lt;/em&gt;&lt;br&gt;
&amp;ndash; from &lt;a href=&#34;https://blogs.kde.org/node/1445&#34;&gt;https://blogs.kde.org/node/1445&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;quot;This has been a long-standing complaint with Java, but it&amp;rsquo;s largely meaningless&amp;hellip;With a Java program, it&amp;rsquo;s far more important to pay attention to what&amp;rsquo;s happening in the heap. The total amount of space consumed is important, and there are some steps that you can take to reduce that. More important is the amount of time that you spend in garbage collection, and which parts of the heap are getting collected.&amp;ldquo;&lt;/em&gt;&lt;br&gt;
&amp;ndash; from &lt;a href=&#34;https://stackoverflow.com/questions/561245/virtual-memory-usage-from-java-under-linux-too-much-memory-used&#34;&gt;https://stackoverflow.com/questions/561245/virtual-memory-usage-from-java-under-linux-too-much-memory-used&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;最后的观点就是，对于java进程来说，我们的关注点不应该是RES，而应该将目光转向heap。查阅很多资料后，能找到的关于这个现象下RES高的最贴切的解释是下面的一段话：&lt;/p&gt;
&lt;p&gt;&lt;b&gt;When is Resident Set Size Important?&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Resident Set size is that portion of the virtual memory space that is actually in RAM. If your RSS grows to be a significant portion of your total physical memory, it might be time to start worrying. If your RSS grows to take up all your physical memory, and your system starts swapping, it&amp;rsquo;s well past time to start worrying.&lt;/p&gt;
&lt;p&gt;But RSS is also misleading, especially on a lightly loaded machine. The operating system doesn&amp;rsquo;t expend a lot of effort to reclaiming the pages used by a process. There&amp;rsquo;s little benefit to be gained by doing so, and the potential for an expensive page fault if the process touches the page in the future. As a result, the RSS statistic may include lots of pages that aren&amp;rsquo;t in active use.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;根据上面这段话，可以看出只有在发生了swap的时候，RES（上文中用RSS表示，二者意义相同）很高才会变得重要，这个时候要考虑是不是有memory leaking这种事情或者其他的情况。在一些轻负载并且尚未发生swap的场景下，高RES占用并不能说明什么问题，可能只是操作系统在reclaiming pages和效率之间的一个权衡，RES中统计的很多内存可能并不是真正在使用。&lt;/p&gt;
&lt;h1 id=&#34;结论&#34;&gt;结论&lt;/h1&gt;
&lt;p&gt;放弃容器级别的metrics转向jvm级别&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Java中如何用wireshark抓取https</title>
            <link rel="alternate" type="text/html" href="https://lsongseven.github.io/posts/java-wireshark-https/" />
            <id>https://lsongseven.github.io/posts/java-wireshark-https/</id>
            <updated>2020-08-28T23:14:27&#43;08:00</updated>
            <published>2020-08-28T22:52:58&#43;08:00</published>
            <author>
                    <name>lsongseven</name>
                    <uri>https://github.com/lsongseven</uri>
                    <email>lsongseven@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">最近遇到的一个问题是，在更换了https的加密协议后（从ssl更换到tls），之前的post请求总是返回400 bad request这样的问题。网上搜罗了一下，有很多人遇到同样的400 bad request，但是各自的具体情况不尽相同，因此没有找到有效的解决方法。
先说一下遇到的情况，对于某一个post请求，在postman中访问没有问题，但是在java代码中使用httpclient按照原先使用正常的方式去做请求，返回的response为一个null，没有其他具体信息，很难追溯。没有办法，只能一步一步跟进调试，一直跟到org.apache.http.impl.execchain包下MainClientExec.class这个类中，并且可以看到请求是正常发送出去，但是收到的response是null。由于对https的了解不是很足，起初怀疑是不是由于更换到tls后握手出现问题，于是调整org.apache.http这个包下的日志等级为debug，可以看到连接是正常建立，因此排除握手的问题。
2020-03-19 16:10:35,119 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:423)]-[DEBUG] Secure session established 2020-03-19 16:10:35,119 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:424)]-[DEBUG] negotiated protocol: TLSv1.2 2020-03-19 16:10:35,119 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:425)]-[DEBUG] negotiated cipher suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 2020-03-19 16:10:35,120 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:433)]-[DEBUG] peer principal: CN=demo.bazefield.com, O=Bazefield A/S, L=Porsgrunn, C=NO 2020-03-19 16:10:35,120 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:442)]-[DEBUG] peer alternative names: [demo.bazefield.com, www.</summary>
            
                <content type="html">&lt;p&gt;最近遇到的一个问题是，在更换了https的加密协议后（从ssl更换到tls），之前的post请求总是返回400 bad request这样的问题。网上搜罗了一下，有很多人遇到同样的400 bad request，但是各自的具体情况不尽相同，因此没有找到有效的解决方法。&lt;/p&gt;
&lt;p&gt;先说一下遇到的情况，对于某一个post请求，在postman中访问没有问题，但是在java代码中使用httpclient按照原先使用正常的方式去做请求，返回的response为一个null，没有其他具体信息，很难追溯。没有办法，只能一步一步跟进调试，一直跟到org.apache.http.impl.execchain包下MainClientExec.class这个类中，并且可以看到请求是正常发送出去，但是收到的response是null。由于对https的了解不是很足，起初怀疑是不是由于更换到tls后握手出现问题，于是调整org.apache.http这个包下的日志等级为debug，可以看到连接是正常建立，因此排除握手的问题。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2020-03-19 16:10:35,119 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:423)]-[DEBUG] Secure session established

2020-03-19 16:10:35,119 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:424)]-[DEBUG] negotiated protocol: TLSv1.2
2020-03-19 16:10:35,119 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:425)]-[DEBUG] negotiated cipher suite: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2020-03-19 16:10:35,120 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:433)]-[DEBUG] peer principal: CN=demo.bazefield.com, O=Bazefield A/S, L=Porsgrunn, C=NO
2020-03-19 16:10:35,120 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:442)]-[DEBUG] peer alternative names: [demo.bazefield.com, www.demo.bazefield.com]
2020-03-19 16:10:35,121 [org.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:446)]-[DEBUG] issuer principal: CN=DigiCert SHA2 Secure Server CA, O=DigiCert Inc, C=US
2020-03-19 16:10:35,126 [org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:145)]-[DEBUG] Connection established 10.77.34.212:52558&amp;lt;-&amp;gt;212.33.148.80:443
2020-03-19 16:10:35,126 [org.apache.http.impl.conn.LoggingManagedHttpClientConnection.setSocketTimeout(LoggingManagedHttpClientConnection.java:90)]-[DEBUG] http-outgoing-0: set socket timeout to 3000
main, setSoTimeout(3000) called
2020-03-19 16:10:35,126 [org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:255)]-[DEBUG] Executing request POST /bazefield.services/api/oauth2/token HTTP/1.1
2020-03-19 16:10:35,126 [org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:260)]-[DEBUG] Target auth state: UNCHALLENGED
2020-03-19 16:10:35,127 [org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:266)]-[DEBUG] Proxy auth state: UNCHALLENGED
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;线索断了之后，只能通过抓包的方式看看究竟发生了什么。这里要出现今天讲的主角wireshark，通过这个强大的工具进行抓包，增加ip和ssl的filter （ip.addr==xxx.yy.zzz.80 and ssl）之后可以看到与目标地址的通讯包。但是，由于是https，包中的流量是加密的，并不能进行分析。于是，如何解密抓取的https流量成为关键。&lt;/p&gt;
&lt;p&gt;解密之前，首先要了解https的基本流程。这个网上有很多描述详细的文章，这边就不赘述。简单说一下，加密是通过指定的加密方法，结合客户端与服务端之间交换的两个随机数，再加上客户端用服务端提供证书上的public key加密的pre-master这个参数（外界是不知道这个参数的，这个参数由客户端生成，经过公钥加密后，只有服务端知道这个参数是什么），生成一个对称加密密钥。在握手结束后，客户端和服务端之间的通讯流量是采用对称加密算法来加密的，而加密的密钥是有握手阶段交换的随机数random_1，random_2以及pre-master和指定的加密算法生成的。这个过程中，最重要的一个参数就是pre-master，其他的参数都是可以通过抓包得到的。因此，如果能够拿到这个pre-master，那么就可以解密wireshark抓取的https流量。&lt;/p&gt;
&lt;p&gt;在前面所说的场景中，我们是客户端，无法知道服务端提供证书的私钥，因此不能从wireshark抓取的包中解密来获得pre-master。但是，我们在代码中所做的请求是通过Apache提供的httpclient完成的，可以在启动时添加vmoption参数-Djavax.net.debug=ssl,keygen来获得tls连接中的一些重要参数。下面是建立连接过程中打印出的部分参数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;*** ECDH ServerKeyExchange
Signature Algorithm SHA512withRSA
Server key: Sun EC public key, 256 bits
public x coord: 82734322831099238985881665140172758067838155879895358323075425906443044790736
public y coord: 88453306709588242368663096476921278570527038619007749416185523277612481734068
parameters: secp256r1 [NIST P-256, X9.62 prime256v1] (1.2.840.10045.3.1.7)
main, READ: TLSv1.2 Handshake, length = 4
check handshake state: server_hello_done[14]
update handshake state: server_hello_done[14]
upcoming handshake states: client certificate[11](optional)
upcoming handshake states: client_key_exchange[16]
upcoming handshake states: certificate_verify[15](optional)
upcoming handshake states: client change_cipher_spec[-1]
upcoming handshake states: client finished[20]
upcoming handshake states: server change_cipher_spec[-1]
upcoming handshake states: server finished[20]
*** ServerHelloDone
*** ECDHClientKeyExchange
ECDH Public value: { 4, 18, 97, 2, 120, 116, 117, 129, 244, 110, 148, 192, 162, 71, 24, 69, 246, 200, 73, 114, 107, 103, 78, 70, 2, 79, 17, 20, 45, 67, 93, 228, 228, 60, 65, 251, 250, 234, 176, 208, 160, 199, 120, 131, 47, 25, 153, 74, 54, 32, 67, 29, 203, 96, 214, 30, 217, 240, 131, 124, 21, 160, 217, 161, 225 }
update handshake state: client_key_exchange[16]
upcoming handshake states: certificate_verify[15](optional)
upcoming handshake states: client change_cipher_spec[-1]
upcoming handshake states: client finished[20]
upcoming handshake states: server change_cipher_spec[-1]
upcoming handshake states: server finished[20]
main, WRITE: TLSv1.2 Handshake, length = 70
SESSION KEYGEN:
PreMaster Secret:
0000: 1B 26 6E 26 56 BB C2 7B C3 37 7B 3F 75 59 EA AC .&amp;amp;n&amp;amp;V....7.?uY..
0010: 25 53 D1 0E 43 F2 85 49 31 FC 4A 98 C5 56 2C E1 %S..C..I1.J..V,.
CONNECTION KEYGEN:
Client Nonce:
0000: 5E 73 28 FA 0F 1A 88 82 6D 91 5D 50 0E BC 6B 87 ^s(.....m.]P..k.
0010: C5 DD 80 83 AB F7 5E 53 45 CB 34 9B C9 CE 16 02 ......^SE.4.....
Server Nonce:
0000: F1 47 01 E9 55 FE 68 35 6F 43 F4 21 80 30 E9 81 .G..U.h5oC.!.0..
0010: 6D 02 E5 DB B8 67 76 39 60 A3 42 D5 1F 43 32 6C m....gv9`.B..C2l
Master Secret:
0000: 8B E2 88 59 42 7A EE F3 5F EC 25 B0 63 6A FC 91 ...YBz.._.%.cj..
0010: 28 2E A3 70 47 63 19 E0 F5 DB 8D 68 64 77 43 91 (..pGc.....hdwC.
0020: 84 A4 A2 2B 1F 87 91 9D 02 76 EB 55 8A 69 65 BC ...+.....v.U.ie.
... no MAC keys used for this cipher
Client write key:
0000: 44 19 21 EE 46 D5 D3 88 06 C1 6B 4A CF 4D A9 36 D.!.F.....kJ.M.6
0010: 21 BA F1 EE 1A C0 A2 60 C2 3B 18 89 32 A3 59 CA !......`.;..2.Y.
Server write key:
0000: 34 C3 40 A7 20 0F 33 9D 19 2E 02 2D 4C 4E 80 30 4.@. .3....-LN.0
0010: 26 25 31 8A 08 DD D5 F0 F4 86 40 03 56 6A F1 D2 &amp;amp;%1.......@.Vj..
Client write IV:
0000: 54 61 92 79 Ta.y
Server write IV:
0000: 15 62 81 55 .b.U
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里我们所要的pre-master就是Clinent Nounce , Master Secret hex值的组合，最后的把它们存在一个sslkey.log文件中，格式如下（手动操作很简单，也可以参考一个python脚本来操作）&lt;/p&gt;
&lt;p&gt;CLIENT_RANDOM 5E731E001D49FA112BAF6B0CFFD8EF5C15DF90B27AE6516DFB83FBA484875ECD 1E53B5AFA6925E8E6B594F48C0BCAF64FDCDE60CA2610EBF6693065A4929E096E8287F9172C1D91CCC8134D2DB1370B4&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##!/usr/bin/env python
import re
import sys


def extract_data_from_line(line):
    m = re.match(&#39;\d+:([ 0-9A-F]{51}) .*&#39;, line)
    if m:
        return m.group(1).replace(&#39; &#39;, &#39;&#39;)
    else:
        raise line


def main():
    f = open(&amp;quot;debug.txt&amp;quot;, &amp;quot;r&amp;quot;)
    parsing_mastersecret_line = 0
    parsing_clientnonce_line = 0
    for line in f.readlines():
        if parsing_mastersecret_line:
            parsing_mastersecret_line += 1
        if parsing_clientnonce_line:
            parsing_clientnonce_line += 1

        if line == &#39;Client Nonce:\n&#39;:
            parsing_clientnonce_line = 1
            cn = &amp;quot;&amp;quot;
        if 2 &amp;lt;= parsing_clientnonce_line &amp;lt;= 3:
            cn = cn + extract_data_from_line(line)

        if line == &#39;Master Secret:\n&#39;:
            parsing_mastersecret_line = 1
            ms = &amp;quot;&amp;quot;
        if 2 &amp;lt;= parsing_mastersecret_line &amp;lt;= 4:
            ms = ms + extract_data_from_line(line)

        if 5 == parsing_mastersecret_line:
            print(&#39;CLIENT_RANDOM&#39;, cn, ms)


if __name__ == &#39;__main__&#39;:
    sys.exit(main())
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;获得了这个重要的pre-master参数后，就可以在wireshark中指定tls的pre-master-secret 文件的名字，于是可以解密https流量。&lt;/p&gt;
&lt;p&gt;回归到要解决的问题，可以发现报出了这样的错误：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Frame 84: 538 bytes on wire (4304 bits), 538 bytes captured (4304 bits) on interface \Device\NPF_{1E884C5A-1654-4DF6-A7E8-01A37930B33B}, id 0
Ethernet II, Src: a2:39:20:00:08:00 (a2:39:20:00:08:00), Dst: 0f:00:08:00:00:00 (0f:00:08:00:00:00)
Internet Protocol Version 4, Src: xxx.yy.zzz.80, Dst: aa.bb.cc.dd
Transmission Control Protocol, Src Port: 443, Dst Port: 63636, Seq: 4343, Ack: 1042, Len: 484
Transport Layer Security
Hypertext Transfer Protocol
HTTP/1.1 400 Bad Request\r\n
[Expert Info (Chat/Sequence): HTTP/1.1 400 Bad Request\r\n]
[HTTP/1.1 400 Bad Request\r\n]
[Severity level: Chat]
[Group: Sequence]
Response Version: HTTP/1.1
Status Code: 400
[Status Code Description: Bad Request]
Response Phrase: Bad Request
Date: Thu, 19 Mar 2020 06:50:21 GMT\r\n
Server: Apache\r\n
Cache-Control: private\r\n
Content-Type: text/html; charset=utf-8\r\n
X-AspNet-Version: 4.0.30319\r\n
X-UA-Compatible: IE=edge\r\n
Vary: Accept-Encoding\r\n
Content-Encoding: gzip\r\n
Content-Length: 168\r\n
[Content length: 168]
Connection: close\r\n
\r\n
[HTTP response 1/1]
[Time since request: 0.325101000 seconds]
[Request in frame: 80]
[Request URI: https://xxx.yyy.com/mmm.services/api/oauth2/token]
Content-encoded entity body (gzip): 168 bytes -&amp;gt; 191 bytes
File Data: 191 bytes
Line-based text data: text/html (1 lines)
Error: SerializationException: Type definitions should start with a &#39;{&#39;, expecting serialized type &#39;TokenRequest&#39;, got string starting with: client_secret=6e18dc44-1171-48b2-8380-66071231831e
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过错误可以猜测到和json的结构有关，于是找到代码中的相关部分进行更正，原来的请求就可以顺利执行了。&lt;/p&gt;
&lt;h1 id=&#34;参考文献&#34;&gt;参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/44786952&#34;&gt;https://zhuanlan.zhihu.com/p/44786952&lt;/a&gt;，&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://timothybasanov.com/2016/05/26/java-pre-master-secret.html&#34;&gt;https://timothybasanov.com/2016/05/26/java-pre-master-secret.html&lt;/a&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">如何在kubernetes中优雅停机</title>
            <link rel="alternate" type="text/html" href="https://lsongseven.github.io/posts/shutdownhook/" />
            <id>https://lsongseven.github.io/posts/shutdownhook/</id>
            <updated>2020-08-28T23:14:33&#43;08:00</updated>
            <published>2020-08-28T22:34:57&#43;08:00</published>
            <author>
                    <name>lsongseven</name>
                    <uri>https://github.com/lsongseven</uri>
                    <email>lsongseven@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">优雅退出 这一篇是为了解决引入buffer后，如何保证数据不丢失？
简单介绍一下问题，如下图，通过open api传入的message会在buffer中囤积到一定量后进行处理，引入buffer是为了减少mysql的压力，减少对于数据库的频繁读写。但是由于buffer是内存中的一个数据结构，那么如何保证数据不丢失呢？
数据丢失的情况可能发生于：（1）升级部署时；（2）pod重启时；（3）极端情况，如断电等。
在公有云情况下，可以先忽略极端情况，云服务商会有一定的保障策略。因此只需要把目光聚集到（1）和（2）两点。 其实可以业界有很多可以参考的例子，比如mysql的redo log和redis的aof都可以一定程度保证在发生问题的时候数据的完整性，但是如果在我们的服务中实现这样的东西太重了，个人认为其实保障数据完整性是一个很通用的需求，应该会有一些开源的项目支持（但是没有找到，不过我认为这是一个赚star的好机会）。
回到主要问题，考虑到我们的服务都是java的服务，有一种简单的方法可以实现在应用退出的时候优雅停机：使用java提供的shutdown hook来做这个事情（在spring boot中也有类似的东西，通过@PreDestroy注解来实现，不过追到底还是shutdown hook），当jvm收到退出的信号时，调用shutdown hook内的方法，完成清理操作。
shutdown hook可以保证在应用主动关闭、代码中调用System#exit、OOM、终端Ctrl+C等情况下被调用，对于我们看重的（1）（2）来说，可以很好的解决问题，举个例子增加如下hook：
Runtime.getRuntime().addShutdownHook(new Thread(() -&amp;gt; System.out.println(&amp;ldquo;shutdown hook!&amp;quot;))); 在退出时会在终端打印出 &amp;ldquo;shutdown hook!&amp;quot;。在实际使用中，可以对将对buffer清理的动作加到hook中，于是可以保证优雅退出。
可是，这样就完了吗？
在kubernetes中优雅停机 在本地java -jar xxx.jar运行一个java程序后，通过kill pid这种方式可以完美实现优雅退出，但是到了ecp上，情况并不是这么回事。在每次重新部署的时候，发现并没有触发shutdown hook，问题出在哪里呢？
这个要从kubernetes中pod关闭说起，参照 https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods
 User sends command to delete Pod, with default grace period (30s) The Pod in the API server is updated with the time beyond which the Pod is considered &amp;ldquo;dead&amp;rdquo; along with the grace period.</summary>
            
                <content type="html">&lt;h1 id=&#34;优雅退出&#34;&gt;优雅退出&lt;/h1&gt;
&lt;p&gt;这一篇是为了解决引入buffer后，如何保证数据不丢失？&lt;/p&gt;
&lt;p&gt;简单介绍一下问题，如下图，通过open api传入的message会在buffer中囤积到一定量后进行处理，引入buffer是为了减少mysql的压力，减少对于数据库的频繁读写。但是由于buffer是内存中的一个数据结构，那么如何保证数据不丢失呢？&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lsongseven.github.io/image/shutdownhook/app-portal-alarm-message.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;数据丢失的情况可能发生于：（1）升级部署时；（2）pod重启时；（3）极端情况，如断电等。&lt;/p&gt;
&lt;p&gt;在公有云情况下，可以先忽略极端情况，云服务商会有一定的保障策略。因此只需要把目光聚集到（1）和（2）两点。 其实可以业界有很多可以参考的例子，比如mysql的redo log和redis的aof都可以一定程度保证在发生问题的时候数据的完整性，但是如果在我们的服务中实现这样的东西太重了，个人认为其实保障数据完整性是一个很通用的需求，应该会有一些开源的项目支持（但是没有找到，不过我认为这是一个赚star的好机会）。&lt;/p&gt;
&lt;p&gt;回到主要问题，考虑到我们的服务都是java的服务，有一种简单的方法可以实现在应用退出的时候优雅停机：使用java提供的shutdown hook来做这个事情（在spring boot中也有类似的东西，通过@PreDestroy注解来实现，不过追到底还是shutdown hook），当jvm收到退出的信号时，调用shutdown hook内的方法，完成清理操作。&lt;/p&gt;
&lt;p&gt;shutdown hook可以保证在应用主动关闭、代码中调用System#exit、OOM、终端Ctrl+C等情况下被调用，对于我们看重的（1）（2）来说，可以很好的解决问题，举个例子增加如下hook：&lt;/p&gt;
&lt;p&gt;Runtime.getRuntime().addShutdownHook(new Thread(() -&amp;gt; System.out.println(&amp;ldquo;shutdown hook!&amp;quot;)));
在退出时会在终端打印出 &amp;ldquo;shutdown hook!&amp;quot;。在实际使用中，可以对将对buffer清理的动作加到hook中，于是可以保证优雅退出。&lt;/p&gt;
&lt;p&gt;可是，这样就完了吗？&lt;/p&gt;
&lt;h1 id=&#34;在kubernetes中优雅停机&#34;&gt;在kubernetes中优雅停机&lt;/h1&gt;
&lt;p&gt;在本地java -jar xxx.jar运行一个java程序后，通过kill pid这种方式可以完美实现优雅退出，但是到了ecp上，情况并不是这么回事。在每次重新部署的时候，发现并没有触发shutdown hook，问题出在哪里呢？&lt;/p&gt;
&lt;p&gt;这个要从kubernetes中pod关闭说起，参照 &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods&#34;&gt;https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;User sends command to delete Pod, with default grace period (30s)&lt;/li&gt;
&lt;li&gt;The Pod in the API server is updated with the time beyond which the Pod is considered &amp;ldquo;dead&amp;rdquo; along with the grace period.&lt;/li&gt;
&lt;li&gt;Pod shows up as &amp;ldquo;Terminating&amp;rdquo; when listed in client commands&lt;/li&gt;
&lt;li&gt;(simultaneous with 3) When the Kubelet sees that a Pod has been marked as terminating because the time in 2 has been set, it begins the Pod shutdown process.
a. If one of the Pod&amp;rsquo;s containers has defined a preStop hook, it is invoked inside of the container. If the preStop hook is still running after the grace period expires, step 2 is then invoked with a small (2 second) one-time extended grace period. You must modify terminationGracePeriodSeconds if the preStop hook needs longer to complete.
b. The container is sent the TERM signal. Note that not all containers in the Pod will receive the TERM signal at the same time and may each require a preStop hook if the order in which they shut down matters.&lt;/li&gt;
&lt;li&gt;(simultaneous with 3) Pod is removed from endpoints list for service, and are no longer considered part of the set of running Pods for replication controllers. Pods that shutdown slowly cannot continue to serve traffic as load balancers (like the service proxy) remove them from their rotations.&lt;/li&gt;
&lt;li&gt;When the grace period expires, any processes still running in the Pod are killed with SIGKILL.&lt;/li&gt;
&lt;li&gt;The Kubelet will finish deleting the Pod on the API server by setting grace period 0 (immediate deletion). The Pod disappears from the API and is no longer visible from the client.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里4.a中提到的preStop hook先不去管他，这是kubernetes提供的一种优雅停机方式。可以看到在4.b中，container会被发送TERM signal，而按照常理来说，如果我们的java进程收到这个TERM信号，那么就会优雅退出，shutdown hook就会执行。可是现实并没有按照我们想要的进行流转，问题就出在这个TERM信号这里。&lt;/p&gt;
&lt;h1 id=&#34;entrypoint中的startupsh&#34;&gt;ENTRYPOINT中的startup.sh&lt;/h1&gt;
&lt;p&gt;仔细查看我们的Dockerfile，大概的样式如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#Dockerfile
...
ENTRYPOINT [&amp;quot;bash&amp;quot;,&amp;quot;./startup.sh&amp;quot;]


#startup.sh
...
nohup java $JAVA_OPTS  -jar ./xxx.jar --server.port=8080 &amp;amp;
while true
do
   sleep 2
done
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这样做的问题是什么呢？&lt;/p&gt;
&lt;p&gt;可以先看一个例子，在容器中，输入ps -ef可以得到&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@xxx-74bc7c554d-vvz4w envuser]# ps -ef
UID         PID   PPID  C STIME TTY          TIME CMD
root          1      0  0 Jul15 ?        00:00:32 bash ./startup.sh
root         10      1  4 Jul15 ?        02:19:06 java -Xmx3072m,output=tcpse
root      94141      0  1 02:55 pts/0    00:00:00 bash
root      94157      1  0 02:55 ?        00:00:00 sleep 2
root      94158  94141  0 02:55 pts/0    00:00:00 ps -ef
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看到，1号进程是我们的bash ./startup.sh，而10号进程是真正服务所在的java进程，并且他是1号进程的子进程。问题就在这里，在pod关闭时，TERM信号只会发送给1号进程，而1号进程并不会将此TERM信号转发到下面的子进程，由此java进程收不到TERM信号，那么也就无法触发shutdown hook，无法优雅退出（最终应该时通过kill -9 pid这种方式退出的）。&lt;/p&gt;
&lt;p&gt;那么现在的问题就在于如何将TERM信号传到java进程上，有两种方法：&lt;/p&gt;
&lt;p&gt;第一种，使用在startup.sh中，使用下面的方式启动java进程，这里exec的作用是使java进程取代当前的bash进程作为1号进程，那么当TERM信号发送给1号进程的时候自然能够优雅退出。但是，这会带来一些问题，我们后面再说。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#startup.sh
...
exec java $JAVA_OPTS -jar ./xxx.jar --server.port=8080 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;第二种，也是推荐的一种，在startup.sh中利用trap捕获TERM信号，将其传递给下面的子进程，如下所示，至于为什么这么写，有一篇很好的参考文献 &lt;a href=&#34;http://veithen.io/2014/11/16/sigterm-propagation.html&#34;&gt;http://veithen.io/2014/11/16/sigterm-propagation.html&lt;/a&gt;：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#startup.sh
...
trap &#39;kill -TERM $child&#39; TERM
nohup java $JAVA_OPTS -jar ./xxx.jar --server.port=8080 &amp;amp;

child=$!
wait $child
wait $child
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;为什么不推荐java进程作为1号进程&#34;&gt;为什么不推荐java进程作为1号进程&lt;/h1&gt;
&lt;p&gt;上面的两种方式里，说到了不推荐第一种方式，为什么呢？&lt;/p&gt;
&lt;p&gt;这要涉及到unix系统中1号进程的特殊作用：1号进程会作为孤儿进程的父进程，同时会需要有收割清理的功能，避免系统产生僵尸进程。对于bash来说，比较完备了，可以很好的adop and reap，然而对于用户写的java进程来说，一般不会考虑到收割清理这种功能，所以如果将java进程作为1号进程的话，容易产生僵尸进程。&lt;/p&gt;
&lt;p&gt;举个例子：&lt;/p&gt;
&lt;p&gt;java (1) -&amp;gt; A process (10)，代表10号进程A process是1号java进程的子进程，当A process 终结的时候，会发送SIGCHILD信号唤醒java进程，期待其进行收割。但是假如java进程没有特殊的处理然后不理会这个信号，那么A process将会成为一个zombie process，即虽然已经终结了，但是依然占据一部分资源，这不是我们所希望看到的。&lt;/p&gt;
&lt;p&gt;因此，推荐使用第二种方式，将bash作为1号进程，这样不必担心zombie process的问题。&lt;/p&gt;
&lt;p&gt;这里再推荐一篇极佳的参考 &lt;a href=&#34;https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/,&#34;&gt;https://blog.phusion.nl/2015/01/20/docker-and-the-pid-1-zombie-reaping-problem/,&lt;/a&gt; 极力推荐大家读一读。&lt;/p&gt;
&lt;h1 id=&#34;最后再说一下while-loop的问题&#34;&gt;最后再说一下while loop的问题&lt;/h1&gt;
&lt;p&gt;在前面之前的startup.sh里面，可以看到最后面有一个while循环，有没有想过为什么要加这个东西？&lt;/p&gt;
&lt;p&gt;其实根源在于它的上一句通过nohup &amp;hellip; &amp;amp;的方式启动了java进程，如果不在最后进行loop，那么startup.sh就会执行完毕而退出，带来的问题就是pod的状态变为completed的状态，也就是说pod认为任务已经执行完毕而自动退出了。作为一个web server端来说，肯定不能让他退出啊，所以在这里采用了while循环来阻止进程退出。不过这是历史问题了，在第二种startup.sh中，trap wait机制会阻塞进程直到子进程退出，因此后面也就不需要while loop这种东西了。最后再留个问题，为什么要两遍wait $child ?那篇参考文献中讲的很详细了 : )&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
        <entry>
            <title type="text">Caffeine内存缓存</title>
            <link rel="alternate" type="text/html" href="https://lsongseven.github.io/posts/caffeine/" />
            <id>https://lsongseven.github.io/posts/caffeine/</id>
            <updated>2020-08-28T22:15:24&#43;08:00</updated>
            <published>2020-08-27T17:47:55&#43;08:00</published>
            <author>
                    <name>lsongseven</name>
                    <uri>https://github.com/lsongseven</uri>
                    <email>lsongseven@gmail.com</email>
                    </author>
            <rights>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</rights><summary type="html">Eviction Policy 在caffeine中缓存的管理使用的是Window TinyLFU(W-TinyLFU)的技术，结构如下图
先来解释一下各个部分：
window cache（绿色部分）: 新对象存入的区域；考虑到缓存的驱逐策略是基于类似LFU的粗略，这里的window cache是对新加入对象的一个保护策略，这些对象会现在window cache中呆一段时间，然后累积一定的frequency之后（LRU策略将其淘汰的时候）才会到TinyLFU中与其他缓存进行frequency的battle，决定去留； main cache （红色和蓝色部分）：大部分缓存所在的区域； TinyLFU (紫色部分，一个admission filter）：当window cache空间存满之后，来自window cache区域的victim将会和来自main cache区域的victim进行比较，判断二者出现频率的大小，如果前者更大，那么将会将前者插入main cache，淘汰掉后者；反之，淘汰掉前者；
下面这个图可以更好的帮助了解Window TinyLFU的过程
在caffeine中，window cache所占大小约为总cache大小的1%，这是一个调参之后的结果，Window TinyLFU的作者做过大量实验证明在大多数的情况下1%能够实现缓存的最大命中率。
在TinyLFU中，使用count min sketch的方式来进行频率的估算，所谓count min sketch，就是利用多个哈希函数来对某个对象的频率进行计数，然后在计算这个对象的频率时，选择最小的哈希函数的计数值最为他的频率（这里可以考虑一种在单一哈希函数下的一个场景，如果有一个热键，他出现的频率很高，哈希之后的键位h1，然后又有一个很冷的键，出现的频率很低，然而由于哈希函数的碰撞其哈希之后的键也为h1，那么这就会造成很难淘汰这个冷键。这里采用多个哈希函数可以很好的规避这种情况）。
count min sketch的结构如下图：
在TinyLFU中，为了使缓存能够有一定的“鲜活性”，在每一次access的时候都会将缓存技术乘以一个aging factor（默认为2），这样的一个操作被称为“Reset”。这样做的好处是可以规避掉历史的“惯性”，举个例子，比如在视频网站上之前一个很火的剧，被点击了上亿次，如果不加以aging的措施的话，可能会一直霸榜，这不是我们希望看到的。
下面再来说一下main cache部分，采用的是Segmented LRU策略，即SLRU。其中会有两个部分：
protected，这部分的对象比较安全，但是如果protected队列满了之后会将lru淘汰入probation队列。 probation，这部分的对象较为危险，如果probation队列满了之后会淘汰lru的对象，但是如果这部分中的对象又再次被访问之后此对象会升级进入protected队列。
Expiration Policy caffeine中的过期策略有两种，分别是fixed expiration policy和variable expiration policy，下面进行简单介绍。</summary>
            
                <content type="html">&lt;h1 id=&#34;eviction-policy&#34;&gt;Eviction Policy&lt;/h1&gt;
&lt;p&gt;在caffeine中缓存的管理使用的是Window TinyLFU(W-TinyLFU)的技术，结构如下图&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lsongseven.github.io/image/caffeine/caffeine.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;先来解释一下各个部分：&lt;/p&gt;
&lt;p&gt;window cache（绿色部分）: 新对象存入的区域；考虑到缓存的驱逐策略是基于类似LFU的粗略，这里的window cache是对新加入对象的一个保护策略，这些对象会现在window cache中呆一段时间，然后累积一定的frequency之后（LRU策略将其淘汰的时候）才会到TinyLFU中与其他缓存进行frequency的battle，决定去留；
main cache （红色和蓝色部分）：大部分缓存所在的区域；
TinyLFU (紫色部分，一个admission filter）：当window cache空间存满之后，来自window cache区域的victim将会和来自main cache区域的victim进行比较，判断二者出现频率的大小，如果前者更大，那么将会将前者插入main cache，淘汰掉后者；反之，淘汰掉前者；&lt;/p&gt;
&lt;p&gt;下面这个图可以更好的帮助了解Window TinyLFU的过程&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lsongseven.github.io/image/caffeine/tiny-lfu.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;在caffeine中，window cache所占大小约为总cache大小的1%，这是一个调参之后的结果，Window TinyLFU的作者做过大量实验证明在大多数的情况下1%能够实现缓存的最大命中率。&lt;/p&gt;
&lt;p&gt;在TinyLFU中，使用count min sketch的方式来进行频率的估算，所谓count min sketch，就是利用多个哈希函数来对某个对象的频率进行计数，然后在计算这个对象的频率时，选择最小的哈希函数的计数值最为他的频率（这里可以考虑一种在单一哈希函数下的一个场景，如果有一个热键，他出现的频率很高，哈希之后的键位h1，然后又有一个很冷的键，出现的频率很低，然而由于哈希函数的碰撞其哈希之后的键也为h1，那么这就会造成很难淘汰这个冷键。这里采用多个哈希函数可以很好的规避这种情况）。&lt;/p&gt;
&lt;p&gt;count min sketch的结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lsongseven.github.io/image/caffeine/count-min-sketch.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;在TinyLFU中，为了使缓存能够有一定的“鲜活性”，在每一次access的时候都会将缓存技术乘以一个aging factor（默认为2），这样的一个操作被称为“Reset”。这样做的好处是可以规避掉历史的“惯性”，举个例子，比如在视频网站上之前一个很火的剧，被点击了上亿次，如果不加以aging的措施的话，可能会一直霸榜，这不是我们希望看到的。&lt;/p&gt;
&lt;p&gt;下面再来说一下main cache部分，采用的是Segmented LRU策略，即SLRU。其中会有两个部分：&lt;/p&gt;
&lt;p&gt;protected，这部分的对象比较安全，但是如果protected队列满了之后会将lru淘汰入probation队列。
probation，这部分的对象较为危险，如果probation队列满了之后会淘汰lru的对象，但是如果这部分中的对象又再次被访问之后此对象会升级进入protected队列。&lt;/p&gt;
&lt;h1 id=&#34;expiration-policy&#34;&gt;Expiration Policy&lt;/h1&gt;
&lt;p&gt;caffeine中的过期策略有两种，分别是fixed expiration policy和variable expiration policy，下面进行简单介绍。&lt;/p&gt;
&lt;p&gt;对于定时过期策略（fixed expiration policy）而言，为每个队列设置一个固定的过期时间（expire after access，expire after write）如60s，同时可维护一个LRU队列，靠近head端的对象为更老的对象，靠近tail端的对象为更新的对象。当某个对象过期时，head朝向tail端移动，这样在进行过期对象清理的时候head左边的对象就是需要清理的，整个操作复杂度在O(1)级别。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lsongseven.github.io/image/caffeine/queue.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于可变过期时间策略(variable expiration policy), 每个对象的过期时间可能不尽相同，caffeine引入时间轮来对此进行操作。关于时间轮的概念可参考kafka中时间轮的设计。&lt;/p&gt;
&lt;p&gt;简单来说，就是利用一个定长数组，每个元素代表一定的时间范围，对应过期时间的缓存对象以双链表的形式存储在这个时间格上。当时间轮转过一圈时，上面一层时间轮每个元素代表的时间范围会是 原始的时间范围x数组元素个数。当时间推动时，时间指针指向的时间格中的对象会被进行过期操作，同时其上层时间轮对应的时间格上的对象会被重新hash填入下层时间轮中。原理其实很简单，可以和生活中的钟表的结构进行类比。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lsongseven.github.io/image/caffeine/time-wheel.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;参考文献&#34;&gt;参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3274808.3274816&#34;&gt;https://dl.acm.org/doi/pdf/10.1145/3274808.3274816&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html&#34;&gt;http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://highscalability.com/blog/2019/2/25/design-of-a-modern-cachepart-deux.html&#34;&gt;http://highscalability.com/blog/2019/2/25/design-of-a-modern-cachepart-deux.html&lt;/a&gt;&lt;/p&gt;
</content>
            
            
            
            
            
                
                    
                
                    
                
            
        </entry>
    
</feed>
